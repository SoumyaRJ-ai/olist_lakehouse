# Olist Lakehouse Project  
**Production-style Medallion Lakehouse implemented with CDC, SCD Type 2, and Incremental Processing using Databricks, Delta Lake, and PySpark with orchestration via Databricks Jobs.**

---

## ğŸ“Œ Overview

This project demonstrates the design and implementation of a production-shaped Data Lakehouse using the Medallion Architecture (Bronze â†’ Silver â†’ Gold) on Databricks.

The objective is not just to ingest data, but to design a pipeline that is:

- Idempotent  
- Incremental  
- Layered with clear responsibility  
- Auditable  
- Modeled using star schema principles  

The dataset represents an e-commerce domain consisting of customers, orders, order items, and products.

---

## ğŸ—ï¸ Architecture Overview

The project follows a Medallion (Bronzeâ€“Silverâ€“Gold) Lakehouse architecture implemented using Databricks and Delta Lake.

![Lakehouse Architecture](architecture/olist_lakehouse_architecture.png)

---

## ğŸ”„ Pipeline Orchestration (Databricks Jobs)

The entire pipeline is orchestrated using Databricks Jobs, with task-level dependency management reflecting upstream and downstream data lineage across Bronze, Silver, and Gold layers.

- Parallel Bronze ingestion
- Layer-based task dependencies
- Retry configuration for failure resilience 
- Fact grain uniqueness is enforced prior to MERGE to prevent ambiguous Delta updates

![Databricks Job DAG](architecture/databricks_job_dag.png)

---

## ğŸ” Data Lineage (Unity Catalog)

Data lineage is traceable across all layers using Unity Catalog, enabling:

- Upstream/downstream dependency tracking  
- Impact analysis  
- Data governance visibility  

![Data Lineage](architecture/lineage_view.png)

---

## ğŸŸ¤ Bronze Layer â€“ Raw Preservation

**Purpose:** Preserve source data faithfully.

**Characteristics:**
- Append-only Delta tables  
- Explicit schema casting  
- Ingestion metadata (`ingestion_ts`, `source_file`)  
- No business logic  

**Tables:**
- customers  
- orders  
- order_items  
- products  

Bronze ensures traceability and serves as the single source of truth.

---

## âšª Silver Layer â€“ Behavioral Modeling

**Purpose:** Model data state and business behavior.

### Orders â†’ CDC (Type 1)
- MERGE-based incremental processing  
- Latest state maintained  
- Idempotent re-runs  

### Customers â†’ SCD Type 2
- Historical tracking of changes  
- `effective_from` / `effective_to` timestamps  
- `is_current` flag  
- Two-step expire-and-insert strategy  

Products and order_items are treated as immutable in this implementation.

---

## ğŸŸ¡ Gold Layer â€“ Analytics Star Schema

**Purpose:** Provide analytics-ready, consumption-safe data.

### Dimensions
- `dim_customers` (SCD Type 2)  
- `dim_products` (Type 1)  

### Fact
- `fact_sales`  
- Grain: `(order_id, order_item_id)`  
- Surrogate keys: `customer_sk`, `product_sk`  
- MERGE-based incremental load  

### Data Quality
- `fact_sales_rejects` captures invalid or filtered records  
- No silent data loss  

Gold enforces a clear separation between descriptive attributes (dimensions) and measurable events (fact).

---

## ğŸ”„ Incremental Processing Strategy

The pipeline is designed to be safely re-runnable.

- Bronze: Append-only ingestion  
- Silver Orders: MERGE-based CDC  
- Silver Customers: Two-step SCD Type 2 implementation  
- Gold Fact: MERGE on composite grain  

All transformations are idempotent and support incremental updates.

---

## ğŸ“Š Data Modeling Highlights

- Clear layer responsibility separation  
- Star schema modeling  
- Strict fact grain definition  
- Surrogate key usage in dimensions  
- Explicit reject handling for data quality transparency  

---

## âš™ Configuration

The project uses a centralized configuration file:

```
config/config.yaml
```

Configuration includes:

- Schema names (bronze, silver, gold)  
- Raw data paths  
- Table names  

This prevents hard-coded values inside transformation scripts and improves maintainability.

---

## ğŸ“‚ Project Structure

```text
olist_lakehouse/
â”‚
â”œâ”€â”€ architecture/                 # Visual documentation
â”‚   â”œâ”€â”€ olist_lakehouse_architecture.png
â”‚   â”œâ”€â”€ databricks_job_dag.png
â”‚   â””â”€â”€ lineage_view.png
â”‚
â”œâ”€â”€ bronze/                       # Raw ingestion layer (append-only)
â”‚   â”œâ”€â”€ bronze_customers.py
â”‚   â”œâ”€â”€ bronze_orders.py
â”‚   â”œâ”€â”€ bronze_products.py
â”‚   â””â”€â”€ bronze_order_items.py
â”‚
â”œâ”€â”€ silver/                       # Business logic layer
â”‚   â”œâ”€â”€ silver_orders_cdc.py      # CDC Type 1 (MERGE-based updates)
â”‚   â””â”€â”€ silver_customers_scd2.py  # SCD Type 2 implementation
â”‚
â”œâ”€â”€ gold/                         # Analytics layer (Star Schema)
â”‚   â”œâ”€â”€ gold_dim_customers.py
â”‚   â”œâ”€â”€ gold_dim_products.py
â”‚   â””â”€â”€ gold_fact_sales.py
â”‚
â”œâ”€â”€ config/                       # Centralized configuration
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ config_loader.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ›  Tech Stack

- Databricks  
- Apache Spark (PySpark)  
- Delta Lake  
- Medallion Architecture  
- Star Schema Modeling  
- Databricks Jobs (Workflow Orchestration)

---

## ğŸ§ª Reproducibility

All configurations are centralized under `config/config.yaml`.  
No hardcoded paths or schema names exist in transformation logic.

---

## âš™ Execution Environment

This project is designed to run within a Databricks environment where a Spark session is already available.

Although the repository is structured as a clean Python project for clarity and modularity, execution assumes:

- An active Spark session  
- Delta Lake support  
- Databricks Runtime  

The `.py` structure improves maintainability and version control, while execution remains cluster-based.

This repository focuses on architectural clarity and production-oriented design rather than local standalone execution.

---

## ğŸš€ Future Enhancements

- Cloud-based incremental ingestion with folder-level detection  
- Partitioning and performance optimization  
- Extended orchestration via Airflow (future scope) 
- Data validation framework  

---

## ğŸ’¡ Design Principles

- Clear layer ownership  
- Idempotent processing  
- Incremental MERGE logic  
- Explicit data quality handling  
- Separation of ingestion, transformation, and analytics concerns  

---

## ğŸ¯ Summary

This project demonstrates how backend engineering rigor can be applied to modern data engineering:

- Deterministic behavior  
- Explicit state management (CDC & SCD2)  
- Controlled schema modeling  
- Transparent data quality handling  

The result is a structured, production-oriented lakehouse implementation rather than a simple ETL script collection.
